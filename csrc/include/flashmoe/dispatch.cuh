//
// Created by osayamen on 1/4/26.
//

#ifndef FLASHMOE_DISPATCH_CUH
#define FLASHMOE_DISPATCH_CUH
#include <cuda/std/bit>
#include <nvshmem.h>

#include "infra/heap.cuh"
#include "infra/packed.cuh"
#include "infra/structures.cuh"
#include "infra/vt.cuh"
namespace flashmoe {
    // MoE tokens dispatch
    template<
        int threads, // not necessarily required, but may help perf a little?
        int bM, // for GEMM0
        int bN, // for GEMM0
        int batch = cute::min(bM, 16), // number of tokens dispatched in one go by a CTA
        DropTokens d = DropTokens::yes,
        typename Element
    >
    __forceinline__ __device__
    void dispatch(const int& H, const int& E,
        const Heap& symHeap,
        const int& EC, const int& roundEC, // EC rounded up to bM
        const int& epRank, const int& world,
        const int& blocks, const Element* __restrict__ const& tokens, // [S, H]
        uint64_t* __restrict__ signals, // [World, globalExpertSlots] but consumer views as [world, num_local_experts]
        const int* __restrict__ const& expertCounts, // [E]
        const TPS* __restrict__ const& _tokenIds, // [E, roundEC] -> generated by ::gate::forward
        int* __restrict__ const& dispatchSync, // [E]
        const PEL* __restrict__ const& expertLookupInfo, // [E]
        cuda::std::byte* __restrict__ const& workspace, // shared memory
        const uint16_t& seqNumber) {
        // Assumptions are below:
        // workspace is in shared memory;
        // Sequence length <= UINT16_MAX * bM, typically for large sequences bM ~ 128, so S <= 8M;
        // tokens and tokensId are 16-byte aligned.
        const int superBlockSize = cute::min(cute::ceil_div(128, cute::max(E, 4)), blocks);
        // we require superblocks to be equally sized
        const auto numSuperBlocks = blocks / superBlockSize;
        const int superBlockIdx = blockIdx.x / superBlockSize;
        if (superBlockIdx >= numSuperBlocks) {
            return;
        }
        const int lBid = blockIdx.x % superBlockSize;
        const bool isLeader = !lBid && !threadIdx.x;
        /// Populate Data Structures
        auto* __restrict__ enL = reinterpret_cast<PEL*>(workspace);
        using V16 = uint4;
        static_assert(sizeof(PEL) % sizeof(V16) == 0);
        auto* __restrict__ cursor = reinterpret_cast<V16*>(workspace);
        const auto* __restrict__ eL = reinterpret_cast<const V16*>(expertLookupInfo);
        const int elElems = static_cast<int>(sizeof(PEL) / sizeof(uint4)) * E;
        for (int i = threadIdx.x; i < elElems; i += threads) {
            cursor[i] = eL[i];
        }
        auto* __restrict__ seC = reinterpret_cast<int*>(workspace + E * sizeof(PEL));
        for (int i = threadIdx.x; i < E; i += threads) {
            seC[i] = expertCounts[i];
        }
        // peer total tokens
        auto* __restrict__ sPTT = seC + E;
        for (int i = threadIdx.x; i < world; i += threads) {
            sPTT[i] = 0;
        }
        __syncthreads();
        for (int i = threadIdx.x; i < E; i += threads) {
            const auto peer = enL[i].peer;
            const auto nTokens = d == DropTokens::yes ? cute::min(seC[i], EC) : seC[i];
            const auto tilesM = cute::ceil_div(nTokens, bM);
            atomicAdd_block(sPTT + peer, tilesM);
        }
        __syncthreads();
        // Update lookup table
        for (int i = threadIdx.x; i < E; i += threads) {
            auto lInfo = enL[i];
            lInfo.eC = seC[i];
            lInfo.pTTt = static_cast<uint16_t>(sPTT[lInfo.peer]);
            enL[i] = lInfo;
        }
        __syncthreads();
        const auto* __restrict__ expertLookup = enL;
        cutlass::AlignedArray<uint32_t, batch> rTID{};
        using VTD = VectorTypeDescriptor<Element, ElementAlignment<Element, bN>>;
        using VectorElement = VTD::VectorType;
        // below is fine because we enforce that H % bN == 0 and bN % vectorWidth == 0
        const int vH = H / VTD::VectorWidth;
        const auto* __restrict__ vTokens = reinterpret_cast<const VectorElement*>(tokens);

        const auto tokenIds = make_tensor(cute::make_gmem_ptr(_tokenIds),
            cute::make_layout(cute::make_shape(E, roundEC), cute::LayoutRight{}));
        static_assert(batch >= 1 && bM % batch == 0);
        for (int expertIdx = superBlockIdx; expertIdx < E; expertIdx += numSuperBlocks) {
            const auto lI = expertLookup[expertIdx];
            const auto flagOffset = epRank * lI.nLocalExperts + lI.expertLocalIdx;
            const auto routedTokens = d == DropTokens::yes ?
                cute::min(lI.eC, EC) : lI.eC;
            auto* __restrict__ peerHeap = reinterpret_cast<VectorElement*>(lI.isRemote ?
                symHeap.advance<0, 0>(lI.peer, lI.expertLocalIdx) :
                lI.remoteSHeap + symHeap.advanceOffset<0, 1>(epRank, lI.expertLocalIdx));
            if (routedTokens) {
                const auto partition = routedTokens / superBlockSize +
                    (lBid < routedTokens % superBlockSize);
                const auto trips = partition / batch;
                for (int i = 0; i < trips; ++i) {
                    // global -> shared
                    #pragma unroll
                    for (int j = 0; j < batch; ++j) {
                        // intentionally redundant copies here to avoid any completion points (CTA barriers) within this loop
                        const auto v = tokenIds(expertIdx, lBid + (j + i * batch) * superBlockSize);
                        // unpack
                        rTID[j] = v.tokenIdx;
                    }
                    // Communicate these tokens
                    #pragma unroll
                    for (int j = 0; j < batch; ++j) {
                        const auto tokenIdx = rTID[j];
                        const auto intraIdx = lBid + (j + i * batch) * superBlockSize;
                        auto* __restrict__ localPH = peerHeap + intraIdx * vH;
                        const auto* __restrict__ aP = vTokens + tokenIdx * vH;
                        // coalesced vectorized copy
                        for (int k = threadIdx.x; k < vH; k += threads) {
                            localPH[k] = aP[k];
                        }
                    }
                }
                // residue
                if (const auto residue = partition - trips * batch; residue) {
                    // global -> shared
                    #pragma unroll
                    for (int j = 0; j < batch; ++j) {
                        if (j < residue) {
                            const auto v = tokenIds(expertIdx, lBid + (j + trips * batch) * superBlockSize);
                            rTID[j] = v.tokenIdx;
                        }
                    }
                    #pragma unroll
                    for (int j = 0; j < batch; ++j) {
                        if (j < residue) {
                            const auto tokenIdx = rTID[j];
                            const auto intraIdx = lBid + (j + trips * batch) * superBlockSize;
                            auto* __restrict__ localPH = peerHeap + intraIdx * vH;
                            const auto* __restrict__ aP = vTokens + tokenIdx * vH;
                            for (int k = threadIdx.x; k < vH; k += threads) {
                                localPH[k] = aP[k];
                            }
                        }
                    }
                }
                __syncthreads();
                if (!threadIdx.x) {
                    cuda::atomic_ref<int, cuda::thread_scope_device> dS{*(dispatchSync + expertIdx)};
                    if (dS.fetch_add(1, cuda::memory_order_acq_rel) + 1 == superBlockSize) {
                        // acq_rel above allows us to safely do the below.
                        // We assume this counter is used _once_ (this function)
                        // per kernel because we defer grid-wide synchronization to kernel teardown.
                        dS.store(0, cuda::memory_order_relaxed);
                        // I am in the last block, let's finalize this transfer.
                        const auto sigPayload = SignalPayload<PacketStage::initial>{
                            routedTokens,
                            lI.pTTt,
                            seqNumber
                        };
                        if (lI.isRemote) {
                            // do RDMA transfer + signal
                            nvshmem_putmem_signal_nbi(
                                symHeap.advance<0, 1>(epRank, lI.expertLocalIdx),
                                peerHeap,
                                sizeof(VectorElement) * routedTokens * vH,
                                signals + flagOffset,
                                cuda::std::bit_cast<uint64_t>(sigPayload),
                                NVSHMEM_SIGNAL_SET,
                                lI.pe);
                        }
                        else {
                            cuda::atomic_ref<uint64_t, cuda::thread_scope_system> sf{*(lI.remoteSFlags + flagOffset)};
                            // we've done the DMA transfer already, so we set the signal only
                            sf.store(cuda::std::bit_cast<uint64_t>(sigPayload), cuda::memory_order_release);
                        }
                    }
                }
            }
            else if (isLeader){
                // single thread sends a noop control message to unblock the remote peer
                // Pack payload into a single signal word
                const auto sigPayload = SignalPayload<PacketStage::initial>{
                    0U, // noop as there are no tokens to be processed
                    lI.pTTt,
                    seqNumber
                };
                if (lI.isRemote) {
                    // transmit signal via RDMA
                    nvshmemx_signal_op(signals + flagOffset,
                        cuda::std::bit_cast<uint64_t>(sigPayload), NVSHMEM_SIGNAL_SET, lI.pe);
                }
                else {
                    cuda::atomic_ref<uint64_t, cuda::thread_scope_system> sf{*(lI.remoteSFlags + flagOffset)};
                    sf.store(cuda::std::bit_cast<uint64_t>(sigPayload), cuda::memory_order_release);
                }
            }
        }
    }
}
#endif //FLASHMOE_DISPATCH_CUH