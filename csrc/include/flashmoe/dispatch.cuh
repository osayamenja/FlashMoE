//
// Created by osayamen on 1/4/26.
//

#ifndef FLASHMOE_DISPATCH_CUH
#define FLASHMOE_DISPATCH_CUH
#include <cuda/std/bit>
#include <cuda/ptx>
#include <nvshmem.h>

#include "infra/heap.cuh"
#include "infra/math.cuh"
#include "infra/packed.cuh"
#include "infra/signal.cuh"
#include "infra/structures.cuh"
#include "infra/vt.cuh"

namespace flashmoe
{

  // MoE tokens dispatch
  template <
    Topology topo,
    int threads, // not necessarily required, but may help perf a little?
    int bM, // for GEMM0
    int bN, // for GEMM0
    int batch = cute::min(bM, 8), // number of tokens dispatched in one go by a CTA
    typename Element
  >
  __forceinline__ __device__
  void dispatch(const int& H, const int& E,
                const Heap& symHeap,
                const int& EC, const int& roundEC, // EC rounded up to bM
                const int& epRank, const int& world,
                const int& superBlockSize, const int& blocks,
                const Element* __restrict__ const& tokens, // [S, H]
                uint64_t* __restrict__ signals, // [world, num_local_experts]
                const int* __restrict__ const& expertCounts, // [E]
                const TPS* __restrict__ const& _tokenIds, // [E, roundEC] -> generated by ::gate::forward
                uint* __restrict__ const& dispatchSync, // [E]
                const PEL* __restrict__ const& expertLookupInfo, // [E]
                cuda::std::byte* __restrict__ const& workspace, // shared memory
                const uint16_t& stateNumber) {
    // Assumptions are below:
    // workspace is in shared memory;
    // ceil(EC, bM) * num_local_experts <= UINT16_MAX
    // tokens and tokensId are at least 32-byte aligned.
    // we require superblocks to be equally sized
    const auto numSuperBlocks = blocks / superBlockSize;
    const int superBlockIdx = blockIdx.x / superBlockSize;
    if (superBlockIdx >= numSuperBlocks) {
      return;
    }
    const int lBid = blockIdx.x % superBlockSize;
    const bool isLeader = !lBid && !threadIdx.x;
    /// Populate Data Structures
    size_t offset = 0;
    auto* __restrict__ enL = reinterpret_cast<PEL*>(workspace);
    for (int i = threadIdx.x; i < E; i += threads) {
      enL[i] = expertLookupInfo[i];
    }
    offset += rTCL<PEL>(E);
    auto* __restrict__ seC = reinterpret_cast<int*>(workspace + offset);
    for (int i = threadIdx.x; i < E; i += threads) {
      seC[i] = expertCounts[i];
    }
    // peer total tokens
    auto* __restrict__ sPTT = seC + E;
    for (int i = threadIdx.x; i < world; i += threads) {
      sPTT[i] = 0;
    }
    __syncthreads();
    for (int i = threadIdx.x; i < E; i += threads) {
      const auto peer = enL[i].peer;
      const auto nTokens = cute::min(seC[i], EC);
      const auto tilesM = cute::ceil_div(nTokens, bM);
      atomicAdd_block(sPTT + peer, tilesM);
    }
    __syncthreads();
    // Update lookup table
    for (int i = threadIdx.x; i < E; i += threads) {
      auto lInfo = enL[i];
      lInfo.eC = seC[i];
      // assert(sPTT[lInfo.peer] <= UINT16_MAX)
      lInfo.pTTt = static_cast<uint16_t>(sPTT[lInfo.peer]);
      enL[i] = lInfo;
    }
    __syncthreads();
    const auto* __restrict__ expertLookup = enL;
    cutlass::AlignedArray<uint32_t, batch> rTID{};
    constexpr auto eWidth = ElementWidth<Element, bN, MAX_ACCESS_ALIGNMENT>; // 256B in Blackwell, 128B others
    using VTD = VectorTypeDescriptor<Element, ElementAlignmentForWidth<Element, bN, eWidth>>;
    using VectorElement = VTD::VectorType;
    // below is fine because we enforce that H % bN == 0 and bN % vectorWidth == 0
    const int vH = H / VTD::VectorWidth::value;
    const auto* __restrict__ vTokens = reinterpret_cast<const VectorElement*>(tokens);

    const auto tokenIds = make_tensor(cute::make_gmem_ptr(_tokenIds),
      cute::make_layout(cute::make_shape(E, roundEC), cute::LayoutRight{}));
    static_assert(batch >= 1 && bM % batch == 0);
    for (int expertIdx = superBlockIdx; expertIdx < E; expertIdx += numSuperBlocks) {
      const auto lI = expertLookup[expertIdx];
      const auto flagOffset = epRank * lI.nLocalExperts + lI.expertLocalIdx;
      // note that for DropTokens:no, we set EC to be the entire sequence length (S) to accommodate the worst-case
      const auto routedTokens = cute::min(lI.eC, EC);
      auto* __restrict__ peerHeap = reinterpret_cast<VectorElement*>(topo == Topology::MIXED && lI.isRemote ?
        symHeap.advance<0, 0>(lI.peer, lI.expertLocalIdx): lI.remoteSHeap +
        symHeap.advanceOffset<0, 1>(epRank, lI.expertLocalIdx));
      if (routedTokens) {
        const auto partition = routedTokens / superBlockSize +
          (lBid < routedTokens % superBlockSize);
        const auto trips = partition / batch;
        for (int i = 0; i < trips; ++i) {
          #pragma unroll
          for (int j = 0; j < batch; ++j) {
            // intentionally redundant copies here to avoid any completion points (CTA barriers) within this loop
            const auto v = tokenIds(expertIdx, lBid + (j + i * batch) * superBlockSize);
            // unpack
            rTID[j] = v.tokenIdx;
          }
          // Communicate these tokens
          #pragma unroll
          for (int j = 0; j < batch; ++j) {
            const auto tokenIdx = rTID[j];
            const auto intraIdx = lBid + (j + i * batch) * superBlockSize;
            auto* __restrict__ localPH = peerHeap + intraIdx * vH;
            const auto* __restrict__ aP = vTokens + tokenIdx * vH;
            // coalesced vectorized copy
            for (int k = threadIdx.x; k < vH; k += threads) {
              const auto v = cuda::ptx::ld_L1_no_allocate(cuda::ptx::space_global, aP + k);
              cuda::ptx::st(cuda::ptx::space_global, localPH + k, v);
            }
          }
        }
        // residue
        if (const auto residue = partition - trips * batch; residue) {
          #pragma unroll
          for (int j = 0; j < batch; ++j) {
            if (j < residue) {
              const auto v = tokenIds(expertIdx, lBid + (j + trips * batch) * superBlockSize);
              rTID[j] = v.tokenIdx;
            }
          }
          #pragma unroll
          for (int j = 0; j < batch; ++j) {
            if (j < residue) {
              const auto tokenIdx = rTID[j];
              const auto intraIdx = lBid + (j + trips * batch) * superBlockSize;
              auto* __restrict__ localPH = peerHeap + intraIdx * vH;
              const auto* __restrict__ aP = vTokens + tokenIdx * vH;
              for (int k = threadIdx.x; k < vH; k += threads) {
                const auto v = cuda::ptx::ld_L1_no_allocate(cuda::ptx::space_global, aP + k);
                cuda::ptx::st(cuda::ptx::space_global, localPH + k, v);
              }
            }
          }
        }
        __syncthreads();
        if (!threadIdx.x) {
          cuda::atomic_ref<uint, cuda::thread_scope_device> dS{*(dispatchSync + expertIdx)};
          if (dS.fetch_add(1, cuda::memory_order_acq_rel) + 1 == superBlockSize) {
            // acq_rel above allows us to safely do the below.
            // We assume this counter is used _once_ (this function)
            // per kernel because we defer grid-wide synchronization to kernel teardown.
            dS.store(0, cuda::memory_order_relaxed);
            // I am in the last block, let's finalize this transfer.
            const auto sigPayload = SignalPayload<PacketStage::initial>{
              routedTokens,
              lI.pTTt,
              stateNumber
            };
            if (topo == Topology::MIXED && lI.isRemote) {
              // do RDMA transfer + signal
              nvshmem_putmem_signal_nbi(
                symHeap.advance<0, 1>(epRank, lI.expertLocalIdx),
                peerHeap,
                sizeof(VectorElement) * routedTokens * vH,
                signals + flagOffset,
                cuda::std::bit_cast<uint64_t>(sigPayload),
                NVSHMEM_SIGNAL_SET,
                lI.pe);
            }
            else {
              cuda::atomic_ref<uint64_t, cuda::thread_scope_system> sf{*(lI.remoteSFlags + flagOffset)};
              // we've done the DMA transfer already, so we set the signal only
              sf.store(cuda::std::bit_cast<uint64_t>(sigPayload), cuda::memory_order_release);
            }
          }
        }
      }
      else if (isLeader) {
        // single thread sends a noop control message to unblock the remote peer
        // Pack payload into a single signal word
        const auto sigPayload = SignalPayload<PacketStage::initial>{
          0U, // noop as there are no tokens to be processed
          lI.pTTt,
          stateNumber
        };
        if (topo == Topology::MIXED && lI.isRemote) {
          // transmit signal via RDMA
          nvshmemx_signal_op(signals + flagOffset,
          cuda::std::bit_cast<uint64_t>(sigPayload), NVSHMEM_SIGNAL_SET, lI.pe);
        }
        else {
          cuda::atomic_ref<uint64_t, cuda::thread_scope_system> sf{*(lI.remoteSFlags + flagOffset)};
          sf.store(cuda::std::bit_cast<uint64_t>(sigPayload), cuda::memory_order_release);
        }
      }
    }
  }
}
#endif //FLASHMOE_DISPATCH_CUH
